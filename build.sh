#!/bin/bash
# ä¸€é”®æ™ºèƒ½æ„å»ºè„šæœ¬ - è‡ªåŠ¨é€‚é…GPUç±»å‹

GPU_TYPE=${1:-auto}

# æ£€æµ‹GPUç±»å‹
detect_gpu() {
    if command -v nvidia-smi >/dev/null 2>&1; then
        GPU_NAME=$(nvidia-smi --query-gpu=name --format=csv,noheader,nounits | head -1)
        case $GPU_NAME in
            *"A100"*|*"H100"*) echo "a100" ;;
            *"RTX 40"*|*"4090"*|*"4080"*) echo "rtx40" ;;
            *) echo "rtx40" ;;  # é»˜è®¤
        esac
    else
        echo "rtx40"  # é»˜è®¤
    fi
}

# ç¦ç”¨BuildKité¿å…å…¼å®¹æ€§é—®é¢˜
export DOCKER_BUILDKIT=0
export COMPOSE_DOCKER_CLI_BUILD=0

# è‡ªåŠ¨æ£€æµ‹GPU
if [ "$GPU_TYPE" = "auto" ]; then
    GPU_TYPE=$(detect_gpu)
fi

echo "ğŸš€ æ£€æµ‹åˆ°GPUç±»å‹: $GPU_TYPE"

# æ ¹æ®GPUç±»å‹è®¾ç½®ç¯å¢ƒå˜é‡
if [ "$GPU_TYPE" = "a100" ]; then
    echo "ğŸ“Š ä½¿ç”¨A100ä¼˜åŒ–é…ç½®..."
    export BASE_IMAGE="nvcr.io/nvidia/pytorch:24.01-py3"
    export SHM_SIZE="32gb"
    export OMP_THREADS="16"
else
    echo "ğŸ® ä½¿ç”¨RTX 40ç³»åˆ—é…ç½®..."
    export BASE_IMAGE="nvcr.io/nvidia/pytorch:23.09-py3"
    export SHM_SIZE="16gb" 
    export OMP_THREADS="8"
fi

# æ„å»º
echo "âš¡ å¼€å§‹æ™ºèƒ½æ„å»º..."
docker-compose build --progress=plain

echo "âœ… Dockeré•œåƒæ„å»ºå®Œæˆï¼"
echo ""
echo "ğŸ“‹ ä¸‹ä¸€æ­¥æ“ä½œ:"
echo "  1. å¯åŠ¨å®¹å™¨: docker-compose up -d train"
echo "  2. è¿›å…¥å®¹å™¨: docker-compose exec train bash"
echo "  3. å¼€å§‹è®­ç»ƒ: python train.py"
echo ""
echo "ğŸ’¡ æç¤º: æ„å»ºå®Œæˆä¸ç­‰äºå¼€å§‹è®­ç»ƒï¼Œéœ€è¦æ‰‹åŠ¨æ‰§è¡Œä¸Šè¿°æ­¥éª¤"