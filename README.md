# ä»é›¶è®­ç»ƒLLM - Dockeræ™ºèƒ½éƒ¨ç½²

## ğŸš€ ä¸€é”®å¯åŠ¨

### å‰ç½®è¦æ±‚
- å®‰è£…Dockerå’ŒDocker Compose
- å®‰è£…NVIDIA Container Toolkitï¼ˆGPUè®­ç»ƒå¿…éœ€ï¼‰
- å‡†å¤‡æ•°æ®é›†æ–‡ä»¶åˆ° `dataset/` ç›®å½•

### å¿«é€Ÿå¼€å§‹
```bash
# 1. å…‹éš†é¡¹ç›®
git clone <your-repo-url>
cd train_llm_from_scratch

# 2. å‡†å¤‡æ•°æ®é›†
mkdir -p dataset
# å°†è®­ç»ƒæ•°æ®æ”¾å…¥datasetç›®å½•

# 3. è‡ªåŠ¨æ£€æµ‹GPUå¹¶æ„å»ºç¯å¢ƒ
./build.sh

# 4. å¯åŠ¨è®­ç»ƒå®¹å™¨ï¼ˆåå°è¿è¡Œï¼‰
docker-compose up -d train

# 5. è¿›å…¥å®¹å™¨å¼€å§‹è®­ç»ƒ
docker-compose exec train bash
python train.py
```

### VSCodeå¼€å‘
```bash
# å®‰è£…VSCodeæ’ä»¶ï¼šDev Containers
# å¯åŠ¨å®¹å™¨åï¼ŒVSCodeä¸­ï¼š
# Ctrl+Shift+P â†’ "Dev Containers: Attach to Running Container"
# é€‰æ‹© "train_llm" å®¹å™¨å³å¯åœ¨å®¹å™¨å†…å¼€å‘
```

## ğŸ“‹ é¡¹ç›®ç‰¹æ€§

- **æ™ºèƒ½GPUé€‚é…**: è‡ªåŠ¨è¯†åˆ«A100/RTX40ç³»åˆ—å¹¶ä¼˜åŒ–é…ç½®
- **å›½å†…é•œåƒåŠ é€Ÿ**: æ¸…åPyPIé•œåƒï¼Œå‘Šåˆ«ä¸‹è½½å¡é¡¿  
- **æ„å»ºç¼“å­˜ä¼˜åŒ–**: BuildKit + åˆ†å±‚ç¼“å­˜ï¼Œé‡å¤æ„å»º2åˆ†é’Ÿå®Œæˆ
- **ä¸€é”®éƒ¨ç½²**: å•ä¸ªè„šæœ¬å®Œæˆæ‰€æœ‰é…ç½®

## ğŸ¯ è®­ç»ƒæµç¨‹

1. **é¢„è®­ç»ƒ**: `python train.py` 
2. **SFTå¾®è°ƒ**: `python sft_train.py`
3. **DPOä¼˜åŒ–**: `python dpo_train.py`

## ğŸ“Š æ€§èƒ½é¢„ä¼° (RTX 4090)

- **é¢„è®­ç»ƒ**: 2-4å°æ—¶
- **SFT**: 1-2å°æ—¶  
- **æ¨¡å‹å‚æ•°**: ~26M
- **æ˜¾å­˜å ç”¨**: ~18GB

## ğŸ”§ è‡ªå®šä¹‰é…ç½®

```bash
# æ‰‹åŠ¨æŒ‡å®šGPUç±»å‹
./build.sh a100     # A100ä¼˜åŒ–é…ç½®
./build.sh rtx40    # RTX40ç³»åˆ—é…ç½®

# ç¯å¢ƒå˜é‡è‡ªå®šä¹‰
export CUDA_DEVICES="0,1"    # å¤šå¡è®­ç»ƒ
export SHM_SIZE="32gb"       # æ›´å¤§å…±äº«å†…å­˜
docker-compose up -d train
```

## ğŸ“ ç›®å½•ç»“æ„

```
â”œâ”€â”€ train.py          # é¢„è®­ç»ƒè„šæœ¬
â”œâ”€â”€ sft_train.py      # SFTå¾®è°ƒè„šæœ¬  
â”œâ”€â”€ dpo_train.py      # DPOè®­ç»ƒè„šæœ¬
â”œâ”€â”€ dataset.py        # æ•°æ®åŠ è½½å™¨
â”œâ”€â”€ tokenizer/        # åˆ†è¯å™¨æ–‡ä»¶
â”œâ”€â”€ Dockerfile        # æ™ºèƒ½GPUé€‚é…é•œåƒ
â”œâ”€â”€ docker-compose.yml # å®¹å™¨ç¼–æ’
â”œâ”€â”€ build.sh          # ä¸€é”®æ„å»ºè„šæœ¬
â””â”€â”€ requirements.txt  # Pythonä¾èµ–
```

## ğŸ› ï¸ Dockerå¸¸ç”¨å‘½ä»¤

### å®¹å™¨ç®¡ç†
```bash
# æŸ¥çœ‹å®¹å™¨çŠ¶æ€
docker-compose ps

# å¯åŠ¨å®¹å™¨
docker-compose up -d train

# åœæ­¢å®¹å™¨
docker-compose down

# è¿›å…¥è¿è¡Œä¸­çš„å®¹å™¨
docker-compose exec train bash

# æŸ¥çœ‹å®¹å™¨æ—¥å¿—
docker-compose logs -f train
```

### æ„å»ºå’Œæ¸…ç†
```bash
# é‡æ–°æ„å»ºé•œåƒ
./build.sh
# æˆ–
docker-compose build --no-cache

# æŸ¥çœ‹é•œåƒå¤§å°
docker images

# æ¸…ç†Dockerç¼“å­˜
docker system prune -f

# æ¸…ç†æ‰€æœ‰æœªä½¿ç”¨çš„èµ„æº
docker system prune -a
```

## ğŸ› ï¸ æ•…éšœæ’é™¤

### GPUç›¸å…³
```bash
# ä¸»æœºGPUæ£€æŸ¥
nvidia-smi

# å®¹å™¨å†…GPUæ£€æŸ¥
docker-compose exec train nvidia-smi

# æµ‹è¯•PyTorch GPU
docker-compose exec train python -c "import torch; print(torch.cuda.is_available())"
```

### ç½‘ç»œé—®é¢˜
```bash
# å¦‚æœDocker Hubè¿æ¥æ…¢ï¼Œå·²é…ç½®å›½å†…é•œåƒåŠ é€Ÿ
# æ£€æŸ¥é•œåƒæºé…ç½®
cat /etc/docker/daemon.json
```

## ğŸ“ˆ ç›‘æ§è®­ç»ƒ

- **TensorBoard**: http://localhost:6006
- **å®æ—¶æ—¥å¿—**: `docker-compose logs -f train`
- **GPUç›‘æ§**: `watch -n 1 nvidia-smi`